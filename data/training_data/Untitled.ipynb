{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60b4a1-bb50-4457-aaba-40ba56a738af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autogoal\n",
    "!pip install autogoal[wikipedia]\n",
    "!pip install autogoal[sklearn]\n",
    "!pip install autogoal[gensim]\n",
    "!pip install autogoal[transformers]\n",
    "!pip install autogoal[keras]\n",
    "!pip install autogoal[spacy]\n",
    "!pip install WordCloud\n",
    "!pip install NLTK\n",
    "!pip install plotly\n",
    "\n",
    "\n",
    "import nltk\n",
    "from wordcloud import STOPWORDS\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# para pre-procesamiento del texto y extraer características\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "\n",
    "stop_words_sp = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adfe881-529a-422c-ae92-1164c3b2b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#  para construir gráficas y realizar análisis exploratorio de los datos\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "\n",
    "# para guardar el modelo\n",
    "import pickle\n",
    "\n",
    "# para construir pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# para evaluar los modelos \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# algoritmos de clasificación\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "TEXT_COL      = 'tweet'\n",
    "CLASS_COL_ST1 = 'Sentiment'\n",
    "CLASS_COL_ST2 = ['Theft', 'Homicide', 'Kidnapping', 'Accident', 'None of the above']\n",
    "\n",
    "# Declaramos algunas variables globales\n",
    "N_JOBS = 6 # Número de núclos a implementar por gridsearch para el hyper parámeter tuning\n",
    "CV = 5 # Número de interaciones para hacer cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0f2c5-03a5-4ea9-8c09-4458a58ea4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# función auxiliar utilizada por CountVectorizer para procesar las frases\n",
    "def spanish_stemmer(sentence):\n",
    "    stemmer = SpanishStemmer()\n",
    "    analyzer = CountVectorizer(binary=False, analyzer='word', stop_words=stop_words_sp,\n",
    "                               ngram_range=(1, 1)).build_analyzer()\n",
    "    return (stemmer.stem(word) for word in analyzer(sentence))\n",
    "\n",
    "\n",
    "# guarda un pipeline entrenado\n",
    "def save_model(model, modelName = \"pickle_model.pkl\"):\n",
    "   pkl_filename = modelName\n",
    "   with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)   \n",
    "\n",
    "\n",
    "# carga un pipeline entrenado y guardado previamente\n",
    "def load_model(rutaModelo = \"pickle_model.pkl\"):\n",
    "  # Load from file\n",
    "  with open(rutaModelo, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    return pickle_model \n",
    "\n",
    "\n",
    "# función auxiliar para realizar predicciones con el modelo\n",
    "def predict_model(model, data, pref='m'):\n",
    "  \"\"\"\n",
    "  data: list of the text to predict\n",
    "  pref: identificador para las columnas (labels_[pref], scores_[pref]_[class 1], etc.)\n",
    "  \"\"\"\n",
    "  res = {}\n",
    "  scores = None\n",
    "  labels = model.predict(data)\n",
    "\n",
    "  if hasattr(model, 'predict_proba'):\n",
    "    scores = model.predict_proba(data)\n",
    "  \n",
    "    # empaquetar scores dentro de un diccionario que contiene labels, scores clase 1, scores clase 2, .... El nombre de la clase se normaliza a lowercase\n",
    "    #res = {f'scores_{pref}_{cls.lower()}':score for cls, score in zip(model.classes_, [col for col in scores.T])}\n",
    "    res = {f'scores_{pref}_{cls}':score for cls, score in zip(model.classes_, [col for col in scores.T])}\n",
    "\n",
    "  # añadir datos relativos a la predicción\n",
    "  res[f'labels_{pref}'] = labels\n",
    "\n",
    "  # convertir a dataframe ordenando las columnas primero el label y luego los scores por clase, las clases ordenadas alfabeticamente.\n",
    "  res = pd.DataFrame(res, columns=sorted(list(res.keys())))\n",
    "\n",
    "  return res\n",
    "\n",
    "\n",
    "# función auxiliar que evalúa los resultados de una clasificación\n",
    "def evaluate_model(y_true, y_pred, y_score=None, pos_label='positive'):\n",
    "  \"\"\"\n",
    "  \n",
    "  \"\"\"\n",
    "  print('==== Sumario de la clasificación ==== ')\n",
    "  print(classification_report(y_true, y_pred))\n",
    "\n",
    "  print('Accuracy -> {:.2%}\\n'.format(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "  # graficar matriz de confusión\n",
    "  display_labels = sorted(unique_labels(y_true, y_pred), reverse=True)\n",
    "  cm = confusion_matrix(y_true, y_pred, labels=display_labels)\n",
    "\n",
    "  z = cm[::-1]\n",
    "  x = display_labels\n",
    "  y =  x[::-1].copy()\n",
    "  z_text = [[str(y) for y in x] for x in z]\n",
    "\n",
    "  fig_cm = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
    "\n",
    "  fig_cm.update_layout(\n",
    "      height=400, width=400,\n",
    "      showlegend=True,\n",
    "      margin={'t':150, 'l':0},\n",
    "      title={'text' : 'Matriz de Confusión', 'x':0.5, 'xanchor': 'center'},\n",
    "      xaxis = {'title_text':'Valor Real', 'tickangle':45, 'side':'top'},\n",
    "      yaxis = {'title_text':'Valor Predicho', 'tickmode':'linear'},\n",
    "  )\n",
    "  fig_cm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662a332-e6d3-4b3f-a17f-d4f0310d2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leemos el corpus de tweets y los labels correspondientes a \n",
    "# las tareas 1 de clasificación binaria y clasificación multilabel\n",
    "df_train            = pd.read_csv('https://raw.githubusercontent.com/carlossuazo/davincis-iberlef-2022/main/data/training_data/train_data.csv', header=None, names = [TEXT_COL])\n",
    "df_train_labels_st1 = pd.read_csv('https://raw.githubusercontent.com/carlossuazo/davincis-iberlef-2022/main/data/training_data/train_labels_subtask_1.csv', header=None, names = [CLASS_COL_ST1])\n",
    "df_train_labels_st2 = pd.read_csv('https://raw.githubusercontent.com/carlossuazo/davincis-iberlef-2022/main/data/training_data/train_labels_subtask_2.csv', header=None, names = CLASS_COL_ST2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5bc66-d281-4686-88ce-ab3079a5aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train_lst1 = pd.concat([df_train, df_train_labels_st1], axis = 1)\n",
    "df_train_lst2 = pd.concat([df_train, df_train_labels_st2], axis = 1)\n",
    "display(df_train_lst1)\n",
    "display(df_train_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1bd221-e5fb-41ee-951c-c969cc23537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtener algunas estadísticas sobre los datos\n",
    "categories = sorted(df_train_lst1[CLASS_COL_ST1].unique(), reverse=False)\n",
    "hist= Counter(df_train_lst1[CLASS_COL_ST1]) \n",
    "print(f'Total de instancias -> {df_train_lst1.shape[0]}')\n",
    "\n",
    "print(f'Categorías -> {categories}')\n",
    "print(f'Comentario de ejemplo -> {df_train_lst1[CLASS_COL_ST1][0]}')\n",
    "print(f'Categoría del comentario -> {df_train_lst1[CLASS_COL_ST1][0]}')\n",
    "\n",
    "fig = go.Figure(layout=go.Layout(height=400, width=600))\n",
    "fig.add_trace(go.Bar(x=categories, y=[hist[cat] for cat in categories]))\n",
    "fig.show()\n",
    "# Hacer entrenamiento con el corpus desbalanceado\n",
    "# Probar rellenando los datos de las categorías 0 y 1, cortar los datos de la categoría 2 utilizar la función iloc, traer 1052 de cada categoría \n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
